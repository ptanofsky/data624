---
title: "DATA 624 Assignment 7"
subtitle: "CUNY Fall 2021"
author: "Philip Tanofsky"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r warning=F, message=F}
# Import required R libraries
library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)
library(pls)
library(elasticnet)
library(corrplot)
```

# Exercise 6.2

Developing a model to predict permeability (see Sect. 1.4) could save significant resources for a pharmaceutical company, while at the same time more rapidly identifying molecules that have a sufficient permeability to become a drug:

## Section a

Start R and use these commands to load the data:

```{r warning=F, message=F}
data(permeability)
data(fingerprints)

fp_data <- as.data.frame(fingerprints)

perm_data <- as.data.frame(permeability)
fp_data <- as.data.frame(fingerprints)

#str(perm_data)
#str(fp_data)

#summary(fp_data)
#head(fp_data)
dim(fp_data)

#summary(perm_data)
#head(perm_data)
dim(perm_data)
```


165 observations in `fingerprints` dataset with 1107 features. 165 observations (predictions) in `permeability` dataset with 1 feature, the prediction.

The matrix `fingerprints` contains the 1,107 binary molecular predictors for the 165 compounds, while permeability contains `permeability` response.

## Section b

The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the molecules contain each substructure. Filter out the predictors that have low frequencies using the `nearZeroVar` function from the `caret` package. How many predictors are left for modeling?

```{r  warning=F, message=F}
nzv_cols <- nearZeroVar(fp_data)
length(nzv_cols)

# From: https://stackoverflow.com/questions/28043393/nearzerovar-function-in-caret
if(length(nzv_cols) > 0) fp_data <- fp_data[, -nzv_cols]

dim(fp_data)

fp_data <- fp_data %>%
  mutate_if(is.numeric,as.factor)

# Add the outcome variable to the predictors dataset
fp_data$Permeability <- perm_data$permeability

dim(fp_data)

#str(fp_data)
```

**Answer:** 719 features with near zero variance, thus 388 features remaining for modeling.

## Section c

Split the data into a training and a test set, pre-process the data, and tune a PLS model. How many latent variables are optimal and what is the corresponding re-sampled estimate of $R^2$?

```{r plsFit, warning=F, message=F}
# From textbook: Prior to performing PLS, the predictors should be centered and scaled,
# PLS has one tuning parameter: the number of components to retain.

# Set the random number seed so we can reproduce the results
set.seed(8675309)

#summary(fp_data)

# Use 80% for training
trainingRows <- createDataPartition(fp_data$Permeability, p = .80, list=FALSE)

# Training set
training_data <- fp_data[trainingRows, ]

# Test set
test_data <- fp_data[-trainingRows, ]

# Initial model following example from book
plsFit <- plsr(Permeability ~ ., data=training_data)
plsFit
```

I'll admit, this initial model was not helpful. On to the tuning of the model.

```{r warning=F, message=F}
# Training pls model based on book example
ctrl <- trainControl(method = "cv", number = 10)

# NOTE: No metric parameter defaults to RMSE
plsTune <- train(Permeability ~ .,
                data = training_data,
                method = "pls",
                metric = "Rsquared",
                tuneLength = 20,
                trControl = ctrl,
                preProc = c("center", "scale")) 

plsTune

ggplot(plsTune) + labs(title="PLS Model Component Tuning")
```

First, the data was split - 80% for training and 20% for the test set. The data was pre-processed using the `preProcess` parameter in the `train` function in order to center and scale the predictors. The PLS model was tuned by way of cross-validation performed 10-fold with a tune length of 20.


**Answer:** Apparently, the running of the chunks in Rstudio and knitting the .rmd file are producing slightly different results. The knitted results above show an optimal latent variables count of 8 with a corresponding $R^2$ value of 0.5674378. The best performing RMSE seen above is 10.74673, also for the variable count of 8. When running the chunks in Rstudio, the optimal number of latent variables is 9 based on the corresponding $R^2$ value of 0.5322824. The best performing RMSE seen above is 10.75441, which resulted from the latent variable count of 8.

## Section d

Predict the response for the test set. What is the test set estimate of $R^2$?

```{r warning=F, message=F}
preds_test <- predict(plsTune, test_data) 
postResample(pred=preds_test, obs=test_data$Permeability)
```

**Answer:** For the test set predictions, the estimate of $R^2$ is 0.398143.

## Section e

Try building other models discussed in this chapter. Do any have better predictive performance?

I have attempted models of type:

- Ridge Regression

- Lasso

- Elastic Net

### Ridge Regression Model

```{r warning=F, message=F}
# Ridge regression
# From book: Page 135
# Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, 1, by = 0.1)) 

ridgeRegFit <- train(Permeability ~ .,
                     data = training_data,
                     method = "ridge",
                     metric = "Rsquared",
                     tuneGrid = ridgeGrid,
                     trControl = ctrl,
                     preProc = c("center", "scale"))

ridgeRegFit
ggplot(ridgeRegFit) + labs(title="Ridge Regression Model With Tuning")

preds_test_rr <- predict(ridgeRegFit, test_data) 
postResample(pred=preds_test_rr, obs=test_data$Permeability)
```

The best performing ridge-regression model used a lambda value of 0.5, which resulted:

- $R^2$: 0.5729303

- RMSE: 13.08302

The best RMSE is 12.12525 with lambda of 0.2.

On the test set, the $R^2$ is 0.4319239.

### Lasso Model

```{r warning=F, message=F}
lassoGrid <- data.frame(.fraction = seq(0, 0.5, by=0.05))

lassoFit <- train(Permeability ~ .,
                  data = training_data,
                  method = 'lasso',
                  metric = 'Rsquared',
                  tuneGrid = lassoGrid,
                  trControl = ctrl,
                  preProcess = c('center','scale'))

lassoFit
ggplot(lassoFit) + labs(title="Lasso Model With Tuning")

preds_test_ls <- predict(lassoFit, test_data) 
postResample(pred=preds_test_ls, obs=test_data$Permeability)
```

The best performing lasso model used a fraction value of 0.4, which resulted:

- $R^2$: 0.5286524

- RMSE: 10.63854

The best RMSE is 10.61055 with fraction value of 0.35.

On the test set, the $R^2$ is 0.2644105.

### Elastic Net Model

```{r warning=F, message=F}
# Elastic Net
# From book: Page 136
enetGrid <- expand.grid(.lambda = c(0, 0.01, .1),
                        .fraction = seq(.05, 1, length = 20))

enetFit <- train(Permeability ~ .,
                 data = training_data,
                 method = 'enet',
                 metric = 'Rsquared',
                 tuneGrid = enetGrid,
                 trControl = ctrl,
                 preProcess = c('center','scale'))
enetFit
ggplot(enetFit) + labs(title="Elastic Net Model With Tuning")

preds_test_en <- predict(enetFit, test_data) 
postResample(pred=preds_test_en, obs=test_data$Permeability)
```

The best performing elastic net model used a fraction value of 0.5 and lambda = 0, which resulted:

- $R^2$: 0.5150926

- RMSE: 11.37484

The best RMSE is 11.16564 with fraction value of 0.30 and lambda = 0.01.

On the test set, the $R^2$ is 0.2410341.

## Section f

Would you recommend any of your models to replace the permeability laboratory experiment?

Based solely on the test set estimates of $R^2$, the ridge regression model performed the best with $R^2$ 0.4319239, as compared to $R^2$ 0.398143 from the PLS model. That being said, these results indicate that the models account for less than half of the variation in the dependent variable that is predictable from the predictors. The permeability assay is expensive labor intensive according to the book, but I don't think any of the above models account for enough variation in the dependent variables to be reliable replacements for the laboratory experiment.

-----

# Exercise 6.3

A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1% will boost revenue by approximately one hundred thousand dollars per batch:

## Section a

Start R and use these commands to load the data:

```{r warning=F, message=F}
data(ChemicalManufacturingProcess)

#str(ChemicalManufacturingProcess)
#summary(ChemicalManufacturingProcess)
dim(ChemicalManufacturingProcess)
#head(ChemicalManufacturingProcess)
cmp_data <- as.data.frame(ChemicalManufacturingProcess)

#head(cmp_data)
```

The matrix `processPredictors` contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. Yield contains the percent yield for each run.

Not sure what `processPredictors` actually refers to, but it's not an object from the `AppliedPredictiveModeling` library.

## Section b

A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values (e.g., see Sect. 3.8).

```{r warning=F, message=F}
# Let's identify the missing values first using vis_miss
library(naniar)
library(RANN)
vis_miss(cmp_data)

# Let's try to impute using preprocess function
# And make sure not to transform the 'Yield' column which is the result
cmp_preprocess_data <- preProcess(cmp_data[, -c(1)], method="knnImpute")

cmp_full_data <- predict(cmp_preprocess_data, cmp_data[, -c(1)])
cmp_full_data$Yield <- cmp_data$Yield
vis_miss(cmp_full_data)
# Note: By using knnImpute, all the data has been centered and scaled
```

The initial plot from `vis_miss` indicates missing values. 

The second plot above confirms no missing values in the predictor columns after applying the kNN imputation approach based on the `preProcess` function.

## Section c

Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?

```{r warning=F, message=F}
# Set the random number seed so we can reproduce the results
# Jenny, I got your number
set.seed(8675309)

nzv_cols <- nearZeroVar(cmp_full_data)
length(nzv_cols)
# From: https://stackoverflow.com/questions/28043393/nearzerovar-function-in-caret
if(length(nzv_cols) > 0) cmp_full_data <- cmp_full_data[, -nzv_cols]
dim(cmp_full_data)

trainingRows <- createDataPartition(cmp_full_data$Yield, p = .80, list=FALSE)

# Training set
training_data <- cmp_full_data[trainingRows, ]

# Test set
test_data <- cmp_full_data[-trainingRows, ]

### Start with PLS model as performed in Exercise 6.2

# Training PLS model based on book example
ctrl <- trainControl(method = "cv", number = 10)

# No metric parameter defaults to RMSE
plsTune <- train(Yield ~ .,
                data = training_data,
                method = "pls",
                metric = "Rsquared",
                tuneLength = 20,
                trControl = ctrl,
                preProc = c("center", "scale")) 

plsTune

ggplot(plsTune) + labs(title="PLS Model With Component Tuning")
```

Based on the performance metric of $R^2$, the latent variable count of 4 resulted in 0.5619734.

## Section d

Predict the response for the test set. What is the value of the performance metric and how does this compare with the re-sampled performance metric on the training set?

```{r warning=F, message=F}
preds_test_pls <- predict(plsTune, test_data) 
postResample(pred=preds_test_pls, obs=test_data$Yield)
```

For the test set, the $R^2$ result is 0.5463996, which is quite close to the 0.5619734 value from the training set.

## Section e

Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list?

```{r warning=F, message=F}
var_imp <- varImp(plsTune)

var_imp
```

As the output above indicates, the top 8 results are Manufacturing Process (process predictors) and 12 of the top 20, while the biological predictors are just 8 of the top 20. BiologicalMaterial02, the biological predictor with the highest score, only results in variable importance of 53.90%. The top 5 Manufacturing Process predictors score greater than 75%.

## Section f

Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process?

```{r warning=F, message=F, fig.width=12, fig.height=10}
top10_preds <- var_imp$importance %>% as.data.frame() %>% arrange(desc(Overall)) %>% head(10) %>% rownames()

vars <- c(top10_preds, "Yield")

corr <- cmp_full_data %>% 
  select(vars) %>% cor()

corrplot(corr, method="number")
```

To assess the relationships of the top predictors and the response, I've selected the top 10 predictors and generated a correlation plot along with the response variable `Yield`. The highest correlation with the response variable not surprising is ManufacturingProcess32. Interesting, ManufacturingProcess36, ManufacturingProcess13, and ManufacturingProcess17 have a negative correlation with `Yield`. If the goal is to improve `Yield`, then the predictors with positive correlation would be the variables to focus on, such as ManufacturingProcess32, ManufacturingProcess09, BiologicalMaterial02, and ManufacturingProcess33.

As the premise of the question states "manufacturing process predictors can be changed in the manufacturing process," then the team should focus on improving predictors ManufacturingProcess32, ManufacturingProcess09, and ManufacturingProcess33 and lowering predictors ManufacturingProcess36, ManufacturingProcess13, and ManufacturingProcess17.