---
title: "DATA 624 Assignment 8"
subtitle: "CUNY Fall 2021"
author: "Philip Tanofsky"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r warning=F, message=F}
# Import required R libraries
library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)
library(corrplot)
library(earth)
library(kernlab)
library(mlbench)
library(kableExtra)

# Set seed once for entire file
set.seed(200)
```

# Exercise 7.2

Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:
$$y = 10 sin(\pi x_1x_2) + 20(x_3 − 0.5)^2 +10x_4 +5x_5 +N(0,\sigma^2)$$
where the $x$ values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package `mlbench` contains a function called `mlbench.friedman1` that simulates these data:

```{r warning=F, message=F}
# n = number of patterns to create
# sd = standard deviation of noise
trainingData <- mlbench.friedman1(200, sd = 1)

## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)

## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.

## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x) 
```

Tune several models on these data. For example:

## K-Nearest Neighbors Model

```{r warning=F, message=F}
knnModel <- train(x = trainingData$x,
                  y = trainingData$y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)
knnModel

ggplot(knnModel) + labs(title="nNN Model With Tuning")

varImp(knnModel)
```

```{r warning=F, message=F}
knnPred <-predict(knnModel, newdata = testData$x)

## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = knnPred, obs = testData$y)
```

**Which models appear to give the best performance? Does MARS select the informative predictors (those named `X1`–`X5`)?**

The above output of the kNN model from the book text results in an RMSE of 3.2040595 and $R^2$ of 0.6819919. Based on Professor Nieman's recommendation of a good modeling landing in the range of 0.7 to 0.85, this model is almost good. The top 5 predictor variables of importance are `X1` - `X5`, not in that order though.

Let's see if I can do better with the other model approaches from the textbook.

# Neural Networks

```{r warning=F, message=F}
# The findCorrelation takes a correlation matrix and determines the
# column numbers that should be removed to keep all pair-wise
# correlations below a threshold
tooHigh <- findCorrelation(cor(trainingData$x), cutoff=0.75)
#trainXnnet <- trainingData$x[, -tooHigh]
#testXnnet <- testData$x[, -tooHigh]
tooHigh
```

The `findCorrelation` function results in no column numbers to be removed.

```{r warning=F, message=F}
ctrl <- trainControl(method = "cv", number = 10)

# Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10),
                        # The next option is to use bagging (see the
                        # next chapter) instead of different random
                        # seeds
                        .bag=FALSE)

nnetTune <- train(trainingData$x, trainingData$y,
                  method="avNNet",
                  tuneGrid=nnetGrid,
                  trControl = ctrl,
                  ## Automatically standardize data prior to modeling
                  # and prediction
                  preProc = c("center", "scale"),
                  linout=TRUE,
                  trace=FALSE,
                  MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
                  maxit=500)

# Output tuned model
nnetTune
# Plot trained/tuned model
ggplot(nnetTune) + labs(title="Neural Networks Model With Tuning")

# Make predictions on Test set
nnetPred <-predict(nnetTune, newdata = testData$x)
# Output prediction performance
postResample(pred = nnetPred, obs = testData$y)

varImp(nnetTune)
```

**Result:** The Neural Networks model results in an RMSE of 2.4788220 and $R^2$ of 0.7870229. Certainly an improvement in the $R^2$ compared to the initial kNN model. Again, the top 5 predictor variables of importance are `X1` - `X5`, not in that order though.

## MARS

```{r warning=F, message=F}
# Tune MARS model
# Define the candidate models to test
marsGrid <- expand.grid(.degree = 1:2,
                        .nprune = 2:38)

# Fix the seed so that the results can be reproduced
marsTuned <- train(trainingData$x, 
                   trainingData$y,
                   method = "earth",
                   tuneGrid = marsGrid,
                   trControl = ctrl)
# Output model
marsTuned
# Plot model
ggplot(marsTuned) + labs(title="MARS Model With Tuning")

# Make predictions on Test set
marsPred <-predict(marsTuned, newdata = testData$x)
# Output prediction performance
postResample(pred = marsPred, obs = testData$y)

# Variable importance of MARS model
varImp(marsTuned)
```

**Result:** The MARS model results in an RMSE of 1.2803060 and $R^2$ of 0.9335241 on the test data. Quite an improvement in the $R^2$ compared to the initial kNN model and the neural networks model.

To answer the textbook question ... yes, the MARS model does select the informative predictors (those named `X1`–`X5`). That being said, the variable `X3` provides an overall importance of 0.0.


## Support Vector Machines

```{r warning=F, message=F}
svmRTuned <- train(trainingData$x, trainingData$y,
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   tuneLength=14,
                   trControl = ctrl)
# Output model
svmRTuned
# Output final model
svmRTuned$finalModel

# Plot of SVM model not valuable
#ggplot(svmRTuned) + labs(title="SVM Model With Tuning")

#head(predict(svmRTuned, testData$x))

# Make predictions on Test set
svmPred <-predict(svmRTuned, newdata = testData$x)
# Output prediction performance
postResample(pred = svmPred, obs = testData$y)

varImp(svmRTuned)
```

**Result:** The Support Vector Machines model results in an RMSE of 2.0693470 and $R^2$ of 0.8263551 on the test set. Certainly an improvement in the $R^2$ compared to the initial kNN model and neural networks but not quite as good as the MARS model on the test set. Again, the top 5 predictor variables of importance are `X1` - `X5`, not in that order though.

Overall, the best performing model on the test set is the MARS model with quite impressive $R^2$ of 0.9335241 on the test data. This result indicates the model is able to account for over 93% of the variance in the test data set. Then again, this data is simulated using the same approach for both the training and the test data, so it shouldn't be too surprising that the models do perform well on the data. I believe that by increasing the count of observations in the test set, the evaluation of the models is fair to gauge overall performance.

# Exercise 7.5

Exercise 6.3 describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models.

## Data Setup

The data setup below is appropriately borrowed from my work in the previous assignment. The `set.seed` function is reset to match the value from before.

```{r warning=F, message=F}
# Reset seed from previous assignment
set.seed(8675309) # Jenny, I got your number

# From Exercise 6.3
data(ChemicalManufacturingProcess)
cmp_data <- as.data.frame(ChemicalManufacturingProcess)

# Let's try to impute using preprocess function
# And make sure not to transform the 'Yield' column which is the result
cmp_preprocess_data <- preProcess(cmp_data[, -c(1)], method="knnImpute")

cmp_full_data <- predict(cmp_preprocess_data, cmp_data[, -c(1)])
cmp_full_data$Yield <- cmp_data$Yield

# Identify near zero variance columns for removal
nzv_cols <- nearZeroVar(cmp_full_data)
length(nzv_cols)
# From: https://stackoverflow.com/questions/28043393/nearzerovar-function-in-caret
if(length(nzv_cols) > 0) cmp_full_data <- cmp_full_data[, -nzv_cols]
dim(cmp_full_data)

trainingRows <- createDataPartition(cmp_full_data$Yield, p = .80, list=FALSE)

# Training set
training_data <- cmp_full_data[trainingRows, ]

# Test set
test_data <- cmp_full_data[-trainingRows, ]

# Based on book example
ctrl <- trainControl(method = "cv", number = 10)
```

With the goal of assessing, the nonlinear regression models on the chemical manufacturing process data, I have fit models of type neural networks, MARS, SVM, and kNN, as I did in the first exercise of this assignment.

## Model: Neural Networks

```{r warning=F, message=F}
# The findCorrelation takes a correlation matrix and determines the
# column numbers that should be removed to keep all pair-wise
## correlations below a threshold
training_data_minus_y <- subset(training_data, select=-c(Yield))

tooHigh <- findCorrelation(cor(training_data_minus_y), cutoff=0.75)
trainXnnet <- training_data[, -tooHigh]
testXnnet <- test_data[, -tooHigh]

ctrl <- trainControl(method = "cv", number = 10)

# Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1),
                        size = c(1:10),
                        bag=FALSE)

nnetTune_cmp <- train(Yield ~ .,
                  data = trainXnnet,
                  method="avNNet",
                  tuneGrid=nnetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"),
                  linout=TRUE,
                  trace=FALSE,
                  MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
                  maxit=500)

# Output tuned model
nnetTune_cmp
# Plot trained/tuned model
ggplot(nnetTune_cmp) + labs(title="Neural Networks Model With Tuning")

# Make predictions on Test set
nnetPred <-predict(nnetTune_cmp, newdata = test_data)
# Output prediction performance
nnet_test_perf <- postResample(pred = nnetPred, obs = test_data$Yield)
nnet_test_perf

nnet_var_imp <- varImp(nnetTune_cmp)
nnet_var_imp
```

**Result:** The Neural Networks model results in RMSE of 1.6747770 and $R^2$ of 0.2824588 on the test set. First reaction, not a good $R^2$ value at all.

## Model: MARS

```{r warning=F, message=F}
# Tune MARS model
# Define the candidate models to test
marsGrid <- expand.grid(.degree = 1:2,
                        .nprune = 2:38)
# Fix the seed so that the results can be reproduced
marsTuned_cmp <- train(Yield ~ ., 
                   data = training_data,
                   method = "earth",
                   # Explicitly declare the candidate models to test
                   tuneGrid = marsGrid,
                   trControl = ctrl)
# Output model
marsTuned_cmp
# Plot model
ggplot(marsTuned_cmp) + labs(title="MARS Model With Tuning")

# Make predictions on Test set
marsPred <-predict(marsTuned_cmp, newdata = test_data)
# Output prediction performance
mars_test_perf <- postResample(pred = marsPred, obs = test_data$Yield)
mars_test_perf

# Variable importance of MARS model
mars_var_imp <- varImp(marsTuned_cmp)
mars_var_imp
```

**Result:** The MARS model results in RMSE of 1.2057906 and $R^2$ of 0.6232327 on the test set. Clearly, a much better $R^2$ compared to the Neural Networks model.

## Model: SVM

```{r warning=F, message=F}
svmRTuned_cmp <- train(Yield ~ .,
                   data = training_data,
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   tuneLength=14,
                   trControl = ctrl)
# Output model
svmRTuned_cmp
# Output final model
svmRTuned_cmp$finalModel
# Again, no need to plot SVM model
# ggplot(svmRTuned) + labs(title="SVM Model With Tuning")

# Make predictions on Test set
svmPred <-predict(svmRTuned_cmp, newdata = test_data)
# Output prediction performance
svm_test_perf <- postResample(pred = svmPred, obs = test_data$Yield)
svm_test_perf

svm_var_imp <- varImp(svmRTuned_cmp)
svm_var_imp
```

**Result:** The SVM model results in RMSE of 1.2001067 and $R^2$ of 0.6453839 on the test set. A much better $R^2$ compared to the Neural Networks model, and even a small improvement over the MARS model.

## Model: kNN

```{r warning=F, message=F}
knnModel_cmp <- train(Yield ~ .,
                  data = training_data,
                  method = "knn",
                  preProc = c("center", "scale"),
                  trContorl = ctrl)
knnModel_cmp
ggplot(knnModel_cmp) + labs(title="kNN Model With Tuning")

knnPred <-predict(knnModel_cmp, newdata = test_data)
knn_test_perf <- postResample(pred = knnPred, obs = test_data$Yield)
knn_test_perf

knn_var_imp <- varImp(knnModel_cmp)
knn_var_imp
```

**Result:** The kNN model results in RMSE of 1.4025385and $R^2$ of 0.5479653 on the test set. A better $R^2$ compared to the Neural Networks model, but under-performed compared to the MARS model and SVM model.

## Section a

Which nonlinear regression model gives the optimal re-sampling and test set performance?

```{r warning=F, message=F}
perf_results <- data.frame(NNet=nnet_test_perf, MARS=mars_test_perf, SVM=svm_test_perf, kNN=knn_test_perf)

perf_results %>% t() %>% kable(caption="Comparison of Model Performance", digits=4) %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

**Answer:** As the above table indicates, the SVM model attained the lowest RMSE and the highest $R^2$.

## Section b

Which predictors are most important in the optimal nonlinear regression model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

```{r warning=F, message=F}
svm_var_imp
```

**Answer:** From the SVM model, the optimal nonlinear regression model, the top 20 most important predictors resulted in 13 manufacturing process variables and 7 biological material variables as seen above.

Overall, I believe the manufacturing variables provide more importance than the biological variables in the nonlinear regression model, but I don't believe the manufacturing variables dominate in the same way they did in the linear regression model.

Compared to the results from the linear regression model, the nonlinear regression model for SVM results in two biological variables in the top 10, BiologicalMaterial06 in 2nd and BiologicalMaterial03 in 6th. Also, 4 of the biological variables scored above 53.9%, which was the highest score of a biological predictor from the linear model. These results support the assessment that the manufacturing variables don't dominate the model as much in the nonlinear model as previously seen in the linear model.

Looking at the top 10 lists of the optimal nonlinear and linear models, the top 6 manufacturing predictors from the linear model are also in the top 10 of the SVM nonlinear model. But, interestingly, the 3 biological predictors in the top 10 of the non-linear model are not in the top 10 of the linear model, so the biological predictors don't behave the same between the optimal linear and nonlinear models.

## Section c

Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?

```{r warning=F, message=F, fig.width=12, fig.height=10}
# From the identified predictors unique to the nonlinear regression model.

top_vars <- c("BiologicalMaterial06",
              "BiologicalMaterial03",
              "BiologicalMaterial12",
              "ManufacturingProcess31",
              "ManufacturingProcess29",
              "ManufacturingProcess30",
              "ManufacturingProcess25",
              "ManufacturingProcess18")

featurePlot(cmp_full_data[,top_vars], cmp_full_data$Yield)
```

The above plot shows some outliers in the manufacturing variables which make it quite difficult to evaluate the variable plots. I will remove the outliers with the goal of improving the plots for readability.

```{r warning=F, message=F, fig.width=12, fig.height=10}
top_vars_and_yield <- c(top_vars, "Yield")
#dim(cmp_full_data[,top_vars_and_yield])

data_for_plot <- cmp_full_data[,top_vars_and_yield] %>%
  filter(ManufacturingProcess31 > -5 & ManufacturingProcess29 > -5 &
           ManufacturingProcess30 > -5 & ManufacturingProcess25 > -5 &
           ManufacturingProcess18 > -5)

# Appear to have removed 2 rows if '> -5' and 7 rows removed if '> -1.5'
#dim(data_for_plot)

featurePlot(data_for_plot[,top_vars], data_for_plot$Yield)
```

Turns out 2 observations were excluded to remove the extreme outliers from the manufacturing variables. The biological variables appear to have a distinct positive linear relationship with the `Yield` predictor variable.

**Answer:** By removing the outliers from the plots of the manufacturing variables, we see the manufacturing variables do not have a clear linear relationship with the predictor variable `Yield`. The visual evaluation of the two sets of feature plots supports the argument for the value of the SVM model as applied to the chemical manufacturing process data. The SVM model uses a "framework of robust regression in order to minimize the effect of outliers on the regression equations," according to the textbook. As the textbook also indicates, "when data may contain influential observations, an alternative minimization metric that is less sensitive ... can be used to find the best parameter estimates."

So when considering the optimal nonlinear regression model, the SVM model outperforms the best linear regression model through identification of several important predictors containing outliers otherwise dismissed by the linear regression model. The underlying algorithm of the SVM model performs exactly as it should by minimizing the effect of the outliers. Yes, the optimal linear and nonlinear regression models share many important variables, but the SVM model does identify several unique variables with high importance by minimizing the effect of the extreme outliers.

```{r warning=F, message=F, fig.width=12, fig.height=10}
corr <- cmp_full_data %>% 
  select(top_vars_and_yield) %>% cor()

corrplot(corr, method="number")
```

The numeric correlation plot above does not provide much more insight than already identified by the feature plots. Unsurprisingly, the 3 biological variables are highly correlated with each other and generally the manufacturing variables are highly correlated with each other.

