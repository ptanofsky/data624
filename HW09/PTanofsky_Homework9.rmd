---
title: "DATA 624 Assignment 9"
subtitle: "CUNY Fall 2021"
author: "Philip Tanofsky"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
    number_sections: false
    theme: united
    highlight: tango
---

```{r warning=F, message=F}
# Import required R libraries
library(AppliedPredictiveModeling)
library(tidyverse)
#library(pls)
#library(elasticnet)
#library(corrplot)

# libraries for Chapter 8
library(caret)
library(Cubist)
library(gbm)
library(ipred)
library(party)
library(partykit)
library(randomForest)
library(rpart)
library(RWeka)
library(kableExtra)

# Set seed for assignment
set.seed(200)
```


# Exercise 8.1

Recreate the simulated data from Exercise 7.2:

```{r warning=F, message=F}
library(mlbench)
simulated <-mlbench.friedman1(200, sd = 1)
simulated <-cbind(simulated$x, simulated$y)
simulated <-as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <-"y"

#simulated
```

## Section a

Fit a random forest model to all of the predictors, then estimate the variable importance scores:

```{r warning=F, message=F}
model1 <- randomForest(y ~ ., 
                       data = simulated,
                       importance = TRUE,
                       ntree = 1000)

rfImp1 <- varImp(model1, scale = FALSE)

rfImp1
```

Did the random forest model significantly use the uninformative predictors (`V6` â€“ `V10`)?

**Answer:** No, based on the output of the variable importance above, the uninformative predictors score very close to zero.

**Note:** The importance value of `V1` is 8.83890885, which is the highest score.

## Section b

Now add an additional predictor that is highly correlated with one of the informative predictors. For example:

```{r warning=F, message=F}
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1

cor(simulated$duplicate1, simulated$V1)
```

**Result:** Yes, the correlation between `V1` and `duplicate1` is above 90% ... highly correlated.

Fit another random forest model to these data. Did the importance score for `V1` change? What happens when you add another predictor that is also highly correlated with `V1`?

```{r warning=F, message=F}
model2 <- randomForest(y ~ ., 
                       data = simulated,
                       importance = TRUE,
                       ntree = 1000)

rfImp2 <- varImp(model2, scale = FALSE)

rfImp2
```

**Answer:** The importance value of `V1` decreased to 6.29780744 from 8.83890885 with the addition of variable `duplicate1`. A drop of almost 29%.

```{r warning=F, message=F}
simulated$duplicate2 <- simulated$V1 + rnorm(200) * .1

cor(simulated$duplicate2, simulated$V1)
```

**Result:** The correlation between `V1` and `duplicate2` is also above 93%.

```{r warning=F, message=F}
model3 <- randomForest(y ~ ., 
                       data = simulated,
                       importance = TRUE,
                       ntree = 1000)

rfImp3 <- varImp(model3, scale = FALSE)

rfImp3
```

**Answer:** Again, the importance of `V1` has decreased, this time from 6.29780744 to 5.656397024 with the addition of a second highly correlated variable. 

## Section c

Use the `cforest` function in the `party` package to fit a random forest model using conditional inference trees. The `party` package function `varimp` can calculate predictor importance. The `conditional` argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?

```{r warning=F, message=F}
bagCtrl <- cforest_control(mtry = ncol(simulated) - 1)

model4 <- party::cforest(y ~ ., data = simulated, controls = bagCtrl)

rfImp4_condFalse <- party::varimp(model4, conditional = FALSE)

rfImp4_condTrue <- party::varimp(model4, conditional = TRUE)

rfImp4_condFalse

rfImp4_condTrue
```

**Answer:** When parameter `conditional` is set to FALSE, then the importance for `V1` appears in line with the above models at a value of 7.373153827. When parameter `conditional` is TRUE, however, the importance of `V1` is much lower at  1.0901055795 and does not follow the pattern of the traditional random forest model.

## Section d

Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?

### Boosted Trees

```{r warning=F, message=F}
gbmModel <- gbm(y ~ ., 
                data = simulated, 
                distribution = "gaussian")

summary.gbm(gbmModel)
```

**Answer:** For boosted trees, a slightly different pattern emerges. Due to the two duplicate predictor variables, predictor variable `V1` has the third highest relative influence. The sum of `V1` and the two duplicates totals 26.96% overall influence, which puts the combined total second.  As expected, the uninformative predictors `V6`-`V10` have little to no relative influence.

### Cubist

```{r warning=F, message=F}
pred_vars <- simulated %>% select(-c(y))

cubistMod <- cubist(pred_vars, simulated$y, committees = 100)

#summary(cubistMod)

varImp(cubistMod)
```

**Answer:** The Cubist model results in a different pattern than above, in which `V1` has the clear highest overall importance at 65.5% despite the inclusion of the two duplicate variables. As for the uninformative variables, the scores are close to zero as expected.

-----

# Exercise 8.2

Use a simulation to show tree bias with different granularities.

I've created three vectors, each of 100 values. The `x1` vector contains values from 1-100 inclusive, and thus the highest number of distinct values. The `x2` vector contains values from 1-25 inclusive. The `x3` vector contains values from 1-10 inclusive, and thus the lowest number of distinct values.

```{r warning=F, message=F}
set.seed(200)
x1 <- trunc(runif(100, 1, 100)) # 100 values between 1-100
x2 <- trunc(runif(100, 1, 25))  # 100 values between 1-25
x3 <- trunc(runif(100, 1, 10))  # 100 values between 1-10
y <-  trunc(runif(100, 1, 50)) 

df <- as.data.frame(cbind(y, x1, x2, x3))

rpartTree <- rpart(y ~ ., data = df)

sim_var_imp <- varImp(rpartTree)

sim_var_imp
```

**Answer:** The results of the variable importance does reflect the tree bias expected based on the textbook description "trees suffer from selection bias: predictors with a higher number of distinct values are favored over more granular predictors." Variable `x1` has the highest number of distinct values and then has the highest variable importance, while `x3` has the lowest number of distinct values and results in the lowest variable importance. 

-----

# Exercise 8.3

In stochastic gradient boosting the bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. Figure 8.24 provides the variable importance plots for boosting using two extreme values for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data. The left-hand plot has both parameters set to 0.1, and the right-hand plot has both set to 0.9:

## Section a

Why does the model on the right focus its importance on just the first few of predictors, whereas the model on the left spreads importance across more predictors?

**From page 207 of the textbook,** "The importance profile for boosting has a much steeper importance slope than the one for random forests. This is due to the fact that the trees from boosting are dependent on each other and hence will have correlated structures as the method follows by the gradient. Therefore many of the same predictors will be selected across the trees, increasing their contribution to the importance metric."

**Answer:** With a high learning rate (0.9), the model on the right will make use of fewer predictors because the percentage of the predicted value is much higher each iteration, and as noted above from the textbook, trees from boosting are dependent on each other. The higher learning rate will lead to increased correlation in the tree structures, which results in the importance being concentrated among fewer variables.

With a low learning rate (0.1), the model on the left would be adding a small fraction to the predicted value for each iteration of the predicted value, thus leading to the likely possibility of more predictors showing importance in the overall model despite the correlation of tree structures.

The bagging fraction also plays a role, as the high bagging fraction of 0.9 would dictate the training occurs on essentially the same data each iteration, again leading to a concentration of important variables. A lower bagging fraction allows the possibility of more variables playing a factor with expected differing makeup of the training samples.

## Section b

Which model do you think would be more predictive of other samples?

**Answer:** The model on the left with the lower learning rate and the lower bagging fraction should be more predictive of other samples. The smaller value of the learning rate typically works better. Regarding the bagging fraction, according to the textbook, the recommendation is 0.5, so given the options of 0.1 or 0.9, I think the lower bagging fraction in conjunction with the lower learning rate would be more predictive of the two options.

## Section c

How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24?

**Answer:** I believe by increasing the interaction depth, a two-fold result will occur. One, the more important variables may actually increase in importance and some of the variables may reach importance above zero with an increased interaction depth. Overall, I believe that by increasing the interaction depth, otherwise known as the tree depth, will lower the slope of predictor importance for either model. By increasing the interaction depth, then more nodes would be used to construct the trees. The more nodes should lead to more variables used in the trees, thus resulting in more importance of those variables. With the greater interaction depth, the most important variables should see an increase in importance as they are re-used more frequently across trees. I would expect the interaction depth to have a greater impact on the right-hand plot which currently has the higher slope of predictor importance.

-----

# Exercise 8.7

Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:

## Data Setup

The data setup below is appropriately borrowed from my work in the previous assignment. The `set.seed` function is reset to match the value from the previous assignment

```{r warning=F, message=F}
# Reset seed from previous assignment
set.seed(8675309) # Jenny, I got your number

# From Exercise 6.3
data(ChemicalManufacturingProcess)
cmp_data <- as.data.frame(ChemicalManufacturingProcess)

# Let's try to impute using preprocess function
# And make sure not to transform the 'Yield' column which is the result
cmp_preprocess_data <- preProcess(cmp_data[, -c(1)], method="knnImpute")

cmp_full_data <- predict(cmp_preprocess_data, cmp_data[, -c(1)])
cmp_full_data$Yield <- cmp_data$Yield

# Identify near zero variance columns for removal
nzv_cols <- nearZeroVar(cmp_full_data)
length(nzv_cols)
# From: https://stackoverflow.com/questions/28043393/nearzerovar-function-in-caret
if(length(nzv_cols) > 0) cmp_full_data <- cmp_full_data[, -nzv_cols]
dim(cmp_full_data)

trainingRows <- createDataPartition(cmp_full_data$Yield, p = .80, list=FALSE)

# Training set
training_data <- cmp_full_data[trainingRows, ]

# Test set
test_data <- cmp_full_data[-trainingRows, ]
```

With the goal of assessing tree-based regression models on the chemical manufacturing process data, I have fit models of a single tree, model trees, random forest, boosted trees and cubist.

### Single Tree

Single tree approach based on the CART methodology.

```{r warning=F, message=F}
rpartTune <- train(Yield ~ .,
                   data = training_data,
                   method = "rpart2",
                   tuneLength = 10,
                   trControl = trainControl(method = "cv"))
# Output model
rpartTune
# Plot model
ggplot(rpartTune) + labs(title="Single Tree Model With Tuning")

# Make predictions on Test set
rpartPred <-predict(rpartTune, newdata = test_data)
# Output prediction performance
rpart_test_perf <- postResample(pred = rpartPred, obs = test_data$Yield)
rpart_test_perf

# Variable importance
rpart_var_imp <- varImp(rpartTune)
rpart_var_imp
```

Based on RMSE, the best-performing single tree model based on CART was a tree with a depth of 1.

### Model Trees

The model trees approach uses the rule-based versions of the model, as well as the use of smoothing and pruning. 

```{r warning=F, message=F}
m5Tune <- train(Yield ~ .,
                data = training_data,
                method = "M5",
                trControl = trainControl(method = "cv"),
                control = Weka_control(M = 10))
# Output model
m5Tune
# Plot model
ggplot(m5Tune) + labs(title="Model Trees With Tuning")

# Make predictions on Test set
m5Pred <-predict(m5Tune, newdata = test_data)
# Output prediction performance
m5_test_perf <- postResample(pred = m5Pred, obs = test_data$Yield)
m5_test_perf

# Variable importance
m5_var_imp <- varImp(m5Tune)
m5_var_imp
```

Based on RMSE, the best performance by the model trees approach uses pruned as yes, smoothed as yes and also rules as yes.

### Random Forest

The random forest approach relies on the primary implementation instead of conditional inference trees.

```{r warning=F, message=F}
rfTune <- train(Yield ~ .,
               data = training_data,
               method = "rf",
               tuneLength = 10,
               trControl = trainControl(method = "cv"))

# Output model
rfTune
# Plot model
ggplot(rfTune) + labs(title="Random Forest With Tuning")

# Make predictions on Test set
rfPred <-predict(rfTune, newdata = test_data)
# Output prediction performance
rf_test_perf <- postResample(pred = rfPred, obs = test_data$Yield)
rf_test_perf

# Variable importance
rf_var_imp <- varImp(rfTune)
rf_var_imp
```

Based on RMSE, the best-performing random forest model uses an mtry of 26 which represents the number of predictors that are randomly sampled as candidates for each split.

### Boosted Trees

The boosting regression approach relies on stochastic gradient boosting machines.

```{r warning=F, message=F}
gbmGrid <- expand.grid(interaction.depth = seq(1, 7, by = 2),
                       n.trees = seq(100, 1000, by = 50),
                       shrinkage = c(0.01, 0.1),
                       n.minobsinnode = c(5, 10, 15))

gbmTune <- train(Yield ~ .,
                 data = training_data,
                 method = "gbm",
                 tuneGrid = gbmGrid,
                 verbose = FALSE)

# Output model
gbmTune
# Plot model
ggplot(gbmTune) + labs(title="Boosted Trees With Tuning")

# Make predictions on Test set
gbmPred <-predict(gbmTune, newdata = test_data)
# Output prediction performance
gbm_test_perf <- postResample(pred = gbmPred, obs = test_data$Yield)
gbm_test_perf

# Variable importance
gbm_var_imp <- varImp(gbmTune)
gbm_var_imp
```

Based on RMSE, the best-performing boosted trees model used the parameters n.trees as 1000, interaction.depth as 7, shrinkage as 0.01 and n.minobsinnode as 5.

### Cubist

The cubist approach is a simple rule-based model with a single committee and no instance-based adjustment, as outlined in the textbook.

```{r warning=F, message=F}
cubistTuned <- train(Yield ~ .,
                     data = training_data, 
                     method = "cubist")

# Output model
cubistTuned
# Plot model
ggplot(cubistTuned) + labs(title="Cubist Model With Tuning")

# Make predictions on Test set
cubPred <-predict(cubistTuned, newdata = test_data)
# Output prediction performance
cub_test_perf <- postResample(pred = cubPred, obs = test_data$Yield)
cub_test_perf

# Variable importance
cub_var_imp <- varImp(cubistTuned)
cub_var_imp
```

Based on RMSE, the best-performing cubist model uses a committees value of 20 and neighbors count of 5.

## Section a

Which tree-based regression model gives the optimal re-sampling and test set performance?

```{r warning=F, message=F}
perf_results <- data.frame(Single_Tree=rpart_test_perf,
                           Model_Trees=m5_test_perf,
                           Random_Forest=rf_test_perf,
                           Boosted_Trees=gbm_test_perf,
                           Cubist=cub_test_perf)

perf_results %>% t() %>% 
  kable(caption="Comparison of Model Performance on Test Data", digits=4) %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

**Answer:** As seen in the table above, the cubist model approach performed the best on the test data set with an RMSE of 1.0278 and an $R^2$ of 0.7529. The M5 trees model, primary random forest model and gradient boosted machines model also performed well, each with an $R^2$ above 0.62. The optimal single tree model did not fare as well on the test data, resulting in an $R^2$ of 0.3384. Not a surprising result for the single tree model, as the optimal single tree model on the training data only had a depth of one.

## Section b

Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

**Answer:** The optimal tree-based regression model is the cubist model with important predictors listed below (the same as above).

```{r warning=F, message=F}
cub_var_imp
```

**More Answer:** Of the top 20 important predictors, manufacturing variables account for 13 while the biological variables account for the remaining 7. The 13-7 split is the same breakdown as the optimal nonlinear regression model (SVM). In comparison to the optimal linear model, then neither type of variable dominates as the manufacturing variables dominate in the optimal linear model. Evaluating the actual scores, the manufacturing variables clearly play a dominate role in the model. Only 3 variables score above 40%, and those three are manufacturing. `ManufacturingProcess32` scores 100% and `ManufacturingProcess09` scores 51% indicating the manufacturing variables play a large role in the model. The top 10 variables for both the optimal linear and nonlinear models scored about 50% importance. The tree-based method shows a much higher reliance on just a few variables as compared to the other model types.

**Top Ten Variables of SVM: Nonlinear Regression Model**

```{r warning=F, message=F}
##                        Overall
## ManufacturingProcess32  100.00
## BiologicalMaterial06     85.65
## ManufacturingProcess36   81.77
## ManufacturingProcess09   81.57
## ManufacturingProcess13   79.95
## BiologicalMaterial03     77.35
## ManufacturingProcess17   76.96
## ManufacturingProcess06   69.87
## BiologicalMaterial12     63.49
## ManufacturingProcess11   62.07
```

**Top Ten Variables of PLS: Linear Regression Model**

```{r warning=F, message=F}
##                        Overall
## ManufacturingProcess32  100.00
## ManufacturingProcess09   85.19
## ManufacturingProcess36   84.84
## ManufacturingProcess13   79.27
## ManufacturingProcess17   77.75
## ManufacturingProcess06   64.16
## ManufacturingProcess11   60.24
## ManufacturingProcess33   54.36
## BiologicalMaterial02     53.90
## BiologicalMaterial08     53.64
```

**More Answer:** The top 3 variables of the cubist model (`ManufacturingProcess32`, `ManufacturingProcess09` and `ManufacturingProcess17`) are also found in the top 10 of both the optimal linear and nonlinear models. Interesting though, the cubist model and the SVM model share two biological variables in the top 10, `BiologicalMaterial06` and `BiologicalMaterial03`, while the linear model does not share top-performing biological variables. As noted above, the biggest observation is the large drop in importance after the first variable in the tree-based model as compared to the optimal linear and nonlinear models.

## Section c

Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?

```{r warning=F, message=F}
rpartTune$finalModel

plot(as.party(rpartTune$finalModel), gp=gpar(fontsize=10))
```

**Answer:** As it turns out, the optimal single tree from the trained model above is a tree with a depth of 1. The condition for the single node is unsurprisingly the manufacturing process variable `ManufacturingProcess32`, the most important variable for all of the optimal model types. The decision node determines the terminal node for values equal to and above 0.192, and thus below the 0.192 value. The boxplots of the terminal nodes show that the higher the value of `ManufacturingProcess32` then likely the higher the `Yield` value. As seen from previous assignments, the higher the manufacturing variable value, then generally the higher the resulting yield, and the above plot reiterates that notion with the most important variable. Given the absence of the biological variables, I can't provide any evaluation of those.